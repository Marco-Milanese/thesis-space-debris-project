import torch
import torch.nn as nn
class YoloLoss(nn.Module):

    def __init__(self, S=16):
        super(YoloLoss, self).__init__()
        self.mse = nn.MSELoss(reduction="sum")
        self.S = S

        # lambdas from Yolo paper
        self.lambdaNoObj = 0.5
        self.lambdaCoord = 5


    def forward(self, predictions, target):

        # predictions are the bboxes generated by the model. Shaped [Batch, 5, 16, 16]

        # target is the ground truth bboxes. Shaped [batch, 7, 6]
        #  the tuples are in the [c, x, y, w, h, n] format relative to the cell size, c is the confidence, n the number of the cell

        # each channel of the prediction is a parameter of the bbox [c, x, y, w, h]
        #               channel indexes:                             0, 1, 2, 3, 4

        
        # transforms predictions from [Batch, 5, 16, 16] to [Batch, 256, 5] so that we can use the n of the targets to find the corresponding prediction
        print(f"pred shape: {predictions.shape}")
        predictions = predictions.permute(0, 2, 3, 1).reshape(-1, self.S * self.S, 5)


        batchSize = target.shape[0]

        # nTrgets contains the n of all the targets and it is shaped [B, 7]
        nTargets = target[:, :, 5].to(dtype=torch.int64) 

        # we create a tensor that contains batchSize lists each filled with 7 index numbers [0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1], .....
        batchIndices = torch.arange(batchSize).unsqueeze(1).expand_as(nTargets).to(target.device)
        objPredictions = predictions[batchIndices, nTargets] # shape [5, 7, 5]
        objTargets = target[:, :, :5] # shape [5, 7, 5]

        # Remove the targets and predictions corresponding to the padding 
        #(Since the number of bboxes in each image is not always the same, in the dataloader we add [0, 0, 0, 0, 0, 0] to reach 7, which is the biggest number of objects in the dataset)
        objMask = objTargets[:, :, 0] > 0

        # These operation make the tensor shape [N, 5], where N is the totalnumber of bboxes to use for the loss calculation
        # Even if the batch dimention is not retained , all the elements from each batch are present
        objPredictions = objPredictions[objMask]  
        objTargets = objTargets[objMask]

        objLoss = (
            self.mse(objTargets[:, 1], objPredictions[:, 1]) +
            self.mse(objTargets[:, 2], objPredictions[:, 2]) +
            self.mse(torch.sqrt(torch.abs(objTargets[:, 3]) + 1e-6), torch.sqrt(torch.abs(objPredictions[:, 3])+ 1e-6)) +
            self.mse(torch.sqrt(torch.abs(objTargets[:, 4]) + 1e-6), torch.sqrt(torch.abs(objPredictions[:, 4])+ 1e-6))
        )

        confLoss = self.mse(objTargets[:, 0], objPredictions[:, 0])

        # now we have to define a mask to keep all the predicted bboxes which do not have the center in one of the target cells 
        objMask = torch.zeros(batchSize, self.S * self.S, dtype=torch.bool, device=target.device)
        objMask[batchIndices, nTargets] = True
        noObjMask = ~objMask  # Invert the mask â†’ True where no object is present


        # confidence predictions where no object exists
        noObjConf = predictions[..., 0][noObjMask]
        # ground truth confidence is always 0 since there is no object
        noObjTarget = torch.zeros_like(noObjConf)
        noObjLoss = self.mse(noObjConf, noObjTarget)


        # sum of the losses with the paper's lambdas
        totalLoss = self.lambdaCoord * objLoss + confLoss + self.lambdaNoObj * noObjLoss

        return totalLoss